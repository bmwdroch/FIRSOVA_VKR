# 3.2. Методика предобработки данных

Эффективность моделей машинного обучения в значительной степени зависит от качества данных, используемых для их обучения. В данном разделе описывается методика предобработки данных телекоммуникационной компании для задачи прогнозирования оттока клиентов.

В разделе 3.1 был проведен подробный разведочный анализ признаков, который позволил выявить некоторые проблемы с данными, требующие обработки перед обучением моделей:

- Наличие пропущенных значений в столбце `TotalCharges`
- Категориальные признаки, требующие кодирования
- Числовые признаки с разными масштабами
- Необходимость разделения на обучающую и тестовую выборки
- Наличие несбалансированности классов в целевой переменной

В данном разделе описывается методика предобработки данных, применяемая для решения этих проблем и подготовки данных к обучению моделей машинного обучения.

## 3.2.1. Очистка данных и обработка пропущенных значений

Первым шагом в процессе предобработки данных является их очистка, которая включает обработку пропущенных значений, дубликатов и аномалий. 

### Анализ пропущенных значений

Для выявления пропущенных значений был проведен анализ всех признаков:

```python
# Проверка наличия пропущенных значений
missing_values = df.isnull().sum()
print("Пропущенные значения по столбцам:")
print(missing_values[missing_values > 0])
```

Результат показал, что пропущенные значения присутствуют только в столбце `TotalCharges`, и их количество составляет 11 строк из 7043 (0.16%).

### Обработка пропущенных значений

Для обработки пропущенных значений в столбце `TotalCharges` был применен следующий подход:

```python
# Анализ строк с пропущенными значениями TotalCharges
missing_total_charges = df[df['TotalCharges'].isnull()]
print(missing_total_charges[['tenure', 'MonthlyCharges', 'TotalCharges']])
```

Анализ показал, что все строки с пропущенными значениями `TotalCharges` имеют значение `tenure` (срок обслуживания) равное 0, что указывает на новых клиентов. Логично предположить, что для новых клиентов общая сумма платежей равна 0:

```python
# Заполнение пропущенных значений TotalCharges нулями
df['TotalCharges'] = df['TotalCharges'].fillna(0)

# Проверка успешности обработки пропущенных значений
print("Пропущенные значения после обработки:")
print(df.isnull().sum().sum())  # Должно быть 0
```

### Обработка выбросов

Для выявления и анализа выбросов в числовых признаках были построены боксплоты:

```
Характеристики числовых признаков:
- tenure: имеет равномерное распределение без явных выбросов
- MonthlyCharges: имеет некоторое количество верхних выбросов (плата >100$)
- TotalCharges: имеет большое количество верхних выбросов, что естественно
  для кумулятивного признака (клиенты с длительным сроком обслуживания)
```

Поскольку выбросы в данном случае представляют собой важные бизнес-кейсы (клиенты с высокими ежемесячными платежами или длительным сроком обслуживания), было решено не удалять их, а вместо этого применить масштабирование, устойчивое к выбросам.

## 3.2.2. Преобразование и кодирование категориальных признаков

Для преобразования категориальных признаков в числовой формат, понятный для алгоритмов машинного обучения, был выбран метод one-hot encoding, который создает бинарные признаки для каждого возможного значения категориального признака.

### Анализ категориальных признаков

Сначала были определены все категориальные признаки и их уникальные значения:

```python
# Определение категориальных признаков
categorical_features = df.select_dtypes(include=['object']).columns.tolist()
print("Категориальные признаки:", categorical_features)

# Анализ уникальных значений категориальных признаков
for feature in categorical_features:
    unique_values = df[feature].unique()
    print(f"{feature}: {unique_values}")
```

Результаты анализа показали, что в данных присутствуют следующие категориальные признаки:
- `gender`: 'Female', 'Male'
- `Partner`: 'Yes', 'No'
- `Dependents`: 'Yes', 'No'
- `PhoneService`: 'Yes', 'No'
- `MultipleLines`: 'No phone service', 'No', 'Yes'
- `InternetService`: 'DSL', 'Fiber optic', 'No'
- `OnlineSecurity`: 'No', 'Yes', 'No internet service'
- `OnlineBackup`: 'No', 'Yes', 'No internet service'
- `DeviceProtection`: 'No', 'Yes', 'No internet service'
- `TechSupport`: 'No', 'Yes', 'No internet service'
- `StreamingTV`: 'No', 'Yes', 'No internet service'
- `StreamingMovies`: 'No', 'Yes', 'No internet service'
- `Contract`: 'Month-to-month', 'One year', 'Two year'
- `PaperlessBilling`: 'Yes', 'No'
- `PaymentMethod`: 'Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'
- `Churn`: 'Yes', 'No'

### Кодирование категориальных признаков

Для кодирования категориальных признаков был использован `OneHotEncoder` из библиотеки scikit-learn:

```python
from sklearn.preprocessing import OneHotEncoder

# Создание кодировщика категориальных признаков
categorical_encoder = OneHotEncoder(drop='first', sparse=False)

# Обучение кодировщика на тренировочных данных
categorical_encoder.fit(X_train[categorical_features])

# Получение имен закодированных признаков
encoded_feature_names = []
for i, feature in enumerate(categorical_features):
    categories = categorical_encoder.categories_[i][1:]  # Skip the first category (dropped)
    for category in categories:
        encoded_feature_names.append(f"{feature}_{category}")

# Просмотр имен закодированных признаков
print("Закодированные признаки:", encoded_feature_names)
```

В результате кодирования каждый категориальный признак был преобразован в набор бинарных признаков, где каждый соответствует одному из возможных значений (за исключением первого значения, которое опускается для избегания мультиколлинеарности). Например, признак `gender` с значениями 'Female' и 'Male' был преобразован в один бинарный признак `gender_Male`.

## 3.2.3. Масштабирование числовых признаков

Поскольку числовые признаки имеют разные диапазоны значений, для эффективного обучения моделей необходимо их масштабирование:

```python
from sklearn.preprocessing import StandardScaler

# Определение числовых признаков
numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']

# Создание масштабировщика числовых признаков
numeric_scaler = StandardScaler()

# Обучение масштабировщика на тренировочных данных
numeric_scaler.fit(X_train[numeric_features])

# Просмотр статистик масштабирования
print("Средние значения для масштабирования:", numeric_scaler.mean_)
print("Стандартные отклонения для масштабирования:", numeric_scaler.scale_)
```

```
Результаты масштабирования числовых признаков:
- tenure: преобразован из диапазона [0, 72] в стандартизованные значения
- MonthlyCharges: преобразованы из диапазона [18.25, 118.75] в стандартизованные значения
- TotalCharges: преобразованы из диапазона [0, 8684.8] в стандартизованные значения
```

## 3.2.4. Разделение данных на обучающую и тестовую выборки

Для оценки эффективности моделей машинного обучения важно разделить данные на обучающую и тестовую выборки. В данном проекте было использовано стратифицированное разделение, сохраняющее соотношение классов:

```python
from sklearn.model_selection import train_test_split

# Определение признаков и целевой переменной
X = df.drop('Churn', axis=1)
y = df['Churn'].map({'Yes': 1, 'No': 0})  # Преобразование в бинарные метки

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Проверка размеров полученных выборок
print("Размер обучающей выборки:", X_train.shape)
print("Размер тестовой выборки:", X_test.shape)

# Проверка баланса классов в обеих выборках
print("Распределение классов в обучающей выборке:")
print(y_train.value_counts(normalize=True))
print("Распределение классов в тестовой выборке:")
print(y_test.value_counts(normalize=True))
```

Результаты показали, что данные были успешно разделены на обучающую (70%, 4930 примеров) и тестовую (30%, 2113 примеров) выборки, с сохранением соотношения классов в обеих выборках (примерно 73.5% "не ушли" и 26.5% "ушли").

## 3.2.5. Балансировка классов

Проблема несбалансированности классов (73.5% "не ушли" vs 26.5% "ушли") может негативно влиять на обучение моделей, поэтому на этапе предобработки были рассмотрены методы балансировки:

```python
# Анализ дисбаланса классов
class_counts = y_train.value_counts()
print("Распределение классов (до балансировки):")
print(class_counts)
print("Соотношение классов (мажоритарный / миноритарный):", 
      class_counts[0] / class_counts[1])
```

Для решения проблемы несбалансированности были подготовлены несколько подходов:

1. **Взвешивание классов**: задание веса классов обратно пропорционально их частотам:
   ```python
   from sklearn.utils.class_weight import compute_class_weight
   
   # Вычисление весов классов
   class_weights = compute_class_weight(
       class_weight='balanced',
       classes=np.unique(y_train),
       y=y_train
   )
   
   # Создание словаря весов
   class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
   print("Веса классов:", class_weight_dict)
   ```

2. **SMOTE** (Synthetic Minority Over-sampling Technique): создание синтетических примеров миноритарного класса:
   ```python
   from imblearn.over_sampling import SMOTE
   
   # Применение SMOTE
   smote = SMOTE(random_state=42)
   X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)
   
   # Проверка результата балансировки
   print("Распределение классов после SMOTE:")
   print(pd.Series(y_train_smote).value_counts())
   ```

3. **Комбинированный подход** (SMOTE + Tomek Links): применение перевыборки с последующим удалением граничных примеров:
   ```python
   from imblearn.combine import SMOTETomek
   
   # Применение SMOTETomek
   smote_tomek = SMOTETomek(random_state=42)
   X_train_smote_tomek, y_train_smote_tomek = smote_tomek.fit_resample(X_train_scaled, y_train)
   
   # Проверка результата балансировки
   print("Распределение классов после SMOTETomek:")
   print(pd.Series(y_train_smote_tomek).value_counts())
   ```

На данном этапе предобработки данных была подготовлена основа для различных методов балансировки, конкретный выбор метода осуществлялся на этапе обучения и оптимизации моделей.

## 3.2.6. Создание пайплайна предобработки

Для автоматизации и стандартизации процесса предобработки данных был создан пайплайн с использованием `Pipeline` и `ColumnTransformer` из scikit-learn:

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Определение преобразователей для разных типов признаков
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(drop='first', sparse=False))
])

# Создание композитного преобразователя
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Обучение преобразователя на обучающей выборке
preprocessor.fit(X_train)

# Применение преобразователя к обучающей и тестовой выборкам
X_train_processed = preprocessor.transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Просмотр размеров преобразованных данных
print("Размер обработанной обучающей выборки:", X_train_processed.shape)
print("Размер обработанной тестовой выборки:", X_test_processed.shape)
```

```
Схема пайплайна предобработки:
1. ColumnTransformer
   ├── Числовые признаки (tenure, MonthlyCharges, TotalCharges)
   │   └── StandardScaler: стандартизация (среднее=0, станд.откл.=1)
   │
   └── Категориальные признаки (gender, Partner, Dependents, ...)
       └── OneHotEncoder: создание бинарных признаков
           с опцией drop='first' для избежания мультиколлинеарности
```

## 3.2.7. Сохранение обработанных данных и пайплайна

Для последующего использования при обучении моделей и в веб-приложении, обработанные данные и пайплайн предобработки были сохранены:

```python
import joblib
from pathlib import Path

# Создание директории для сохранения данных (если не существует)
processed_data_dir = Path('data/processed')
processed_data_dir.mkdir(parents=True, exist_ok=True)

models_dir = Path('models')
models_dir.mkdir(exist_ok=True)

# Сохранение обработанных данных
np.save(processed_data_dir / 'X_train_processed.npy', X_train_processed)
np.save(processed_data_dir / 'X_test_processed.npy', X_test_processed)
np.save(processed_data_dir / 'y_train.npy', y_train.values)
np.save(processed_data_dir / 'y_test.npy', y_test.values)

# Сохранение препроцессора
joblib.dump(preprocessor, models_dir / 'preprocessor.joblib')

print("Обработанные данные и препроцессор успешно сохранены!")
```

Сохраненный пайплайн предобработки в дальнейшем используется при разработке веб-приложения для обеспечения однообразного преобразования входных данных.

## 3.2.8. Выводы

В результате предобработки данных были выполнены следующие задачи:

1. Проведена очистка данных и обработка пропущенных значений, что обеспечило полноту и целостность данных для анализа.

2. Выполнено преобразование категориальных признаков с использованием one-hot encoding, что позволило использовать эти признаки в моделях машинного обучения.

3. Произведено масштабирование числовых признаков, что обеспечивает их равнозначность при обучении моделей и улучшает сходимость алгоритмов.

4. Данные были разделены на обучающую и тестовую выборки с сохранением соотношения классов, что обеспечивает объективную оценку качества моделей.

5. Подготовлены методы для балансировки классов, которые будут применяться на этапе обучения и оптимизации моделей.

6. Создан и сохранен пайплайн предобработки, который обеспечивает стандартизацию процесса обработки данных и возможность его повторного использования.

Предобработанные данные теперь готовы для использования на следующем этапе - разработке и обучении моделей машинного обучения для прогнозирования оттока клиентов. 